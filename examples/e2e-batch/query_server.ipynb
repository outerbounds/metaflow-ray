{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: METAFLOW_PROFILE=oleg2\n"
     ]
    }
   ],
   "source": [
    "%env METAFLOW_PROFILE=oleg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the server\n",
    "\n",
    "```\n",
    "serve run server:batch_preds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-28 13:36:38,913\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-10-28 13:36:39,332\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from base import TabularBatchPrediction as TBP\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_world_output = requests.post(\"http://localhost:8000\").json()\n",
    "assert hello_world_output['response'] == \"Hello World!\", \"Are you sure you're running the server?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data() -> pd.DataFrame:\n",
    "    tbp = TBP()\n",
    "    _, valid_dataset, test_dataset = tbp.load_dataset()\n",
    "    df = test_dataset.to_pandas()\n",
    "    df[\"id\"] = df.index\n",
    "    return df, valid_dataset.to_pandas()[\"target\"].values\n",
    "\n",
    "\n",
    "def prepare_post_body(batch: pd.DataFrame) -> dict:\n",
    "    id_to_batch = {}\n",
    "    for record in batch:\n",
    "        _id = record.pop(\"id\")\n",
    "        id_to_batch[_id] = record\n",
    "    return id_to_batch\n",
    "\n",
    "\n",
    "def query_predict_endpoint(batch_size=5, df=None):\n",
    "    if df is None:\n",
    "        df, true_targets = get_data()\n",
    "    batch = df.sample(batch_size).to_dict(\"records\")\n",
    "    id_to_batch_features = prepare_post_body(batch)\n",
    "    output = requests.post(\n",
    "        \"http://localhost:8000/predict/\", data=json.dumps(id_to_batch_features)\n",
    "    ).json()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No dataset name or loader provided. Using default breast_cancer.csv dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-28 13:37:31,584\tINFO read_api.py:374 -- To satisfy the requested parallelism of 20, each read task output will be split into 20 smaller blocks.\n",
      "2023-10-28 13:37:31,997\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "2023-10-28 13:37:31,998\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-10-28 13:37:31,998\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    }
   ],
   "source": [
    "df, true_targets = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"Prediction proba for sample input {} is {}. True target is {}.\"\n",
    "output = query_predict_endpoint(batch_size=5, df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction proba for sample input 64 is 0.8402084112167358. True target is 1.\n",
      "Prediction proba for sample input 158 is 0.8899078965187073. True target is 1.\n",
      "Prediction proba for sample input 15 is 0.47248977422714233. True target is 1.\n",
      "Prediction proba for sample input 25 is 0.8899078965187073. True target is 1.\n",
      "Prediction proba for sample input 35 is 0.12808369100093842. True target is 0.\n"
     ]
    }
   ],
   "source": [
    "for id, proba in output.items():\n",
    "    print(msg.format(id, proba, true_targets[int(id)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is running on the `/predict` endpoint on the server?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 17:47:14,732\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(StandardScaler._transform_pandas)->MapBatches(ScoringWrapper)]\n",
      "2023-09-14 17:47:14,734\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-09-14 17:47:14,735\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-09-14 17:47:14,766\tINFO actor_pool_map_operator.py:117 -- MapBatches(StandardScaler._transform_pandas)->MapBatches(ScoringWrapper): Waiting for 1 pool actors to start...\n",
      "2023-09-14 17:47:15,747\tWARNING actor_pool_map_operator.py:267 -- To ensure full parallelization across an actor pool of size 1, the specified batch size should be at most 2. Your configured batch size for this operator was 4096.\n"
     ]
    }
   ],
   "source": [
    "from ray.train.xgboost import XGBoostPredictor\n",
    "import ray\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from metaflow import Run, Flow\n",
    "\n",
    "\n",
    "def select_from_checkpoint_registry(flow_name=\"Train\"):\n",
    "    flow = Flow(flow_name)\n",
    "    run = flow.latest_successful_run\n",
    "    result = run.data.result\n",
    "    return result.checkpoint\n",
    "\n",
    "\n",
    "# duplicated in `query_predict_endpoint` above\n",
    "batch_size = 2\n",
    "if df is None:\n",
    "    df = get_data()\n",
    "batch = df.sample(batch_size).to_dict(\"records\")\n",
    "id_to_batch_features = prepare_post_body(batch)\n",
    "\n",
    "features = ray.data.from_items(list(id_to_batch_features.values()))\n",
    "checkpoint = select_from_checkpoint_registry()\n",
    "predictor = BatchPredictor.from_checkpoint(checkpoint, XGBoostPredictor)\n",
    "preds = predictor.predict(features).to_pandas()[\"predictions\"].values\n",
    "preds_payload = dict(zip(id_to_batch_features.keys(), preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{54: 0.89597195, 155: 0.89597195}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_payload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
